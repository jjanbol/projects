{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b18e40b-5993-4bda-b4a9-87165cbbd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68845565-fede-4f03-ad03-dbfb0599e5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8d4fdcca-fb4a-414a-860f-6744817963a7.webp\n",
      "\u001b[31mConcrete_Data.xls\u001b[m\u001b[m\n",
      "Project proposal  Part 1 and 2.pdf\n",
      "best_model\n",
      "concrete copy.png\n",
      "concrete.jpg\n",
      "concrete_strength.ipynb\n",
      "neon.png\n",
      "project_with_gui.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "813b55e7-240a-4be1-bf67-0f348ad5e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ingesting the data\n",
    "df = pd.read_excel(\"Concrete_Data.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0df8e07-3e8e-40a5-9289-fc1f780ccf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement (component 1)(kg in a m^3 mixture)</th>\n",
       "      <th>Blast Furnace Slag (component 2)(kg in a m^3 mixture)</th>\n",
       "      <th>Fly Ash (component 3)(kg in a m^3 mixture)</th>\n",
       "      <th>Water  (component 4)(kg in a m^3 mixture)</th>\n",
       "      <th>Superplasticizer (component 5)(kg in a m^3 mixture)</th>\n",
       "      <th>Coarse Aggregate  (component 6)(kg in a m^3 mixture)</th>\n",
       "      <th>Fine Aggregate (component 7)(kg in a m^3 mixture)</th>\n",
       "      <th>Age (day)</th>\n",
       "      <th>Concrete compressive strength(MPa, megapascals)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.887366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.269535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.052780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.296075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>276.4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>179.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>870.1</td>\n",
       "      <td>768.3</td>\n",
       "      <td>28</td>\n",
       "      <td>44.284354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>322.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.6</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>817.9</td>\n",
       "      <td>813.4</td>\n",
       "      <td>28</td>\n",
       "      <td>31.178794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>148.5</td>\n",
       "      <td>139.4</td>\n",
       "      <td>108.6</td>\n",
       "      <td>192.7</td>\n",
       "      <td>6.1</td>\n",
       "      <td>892.4</td>\n",
       "      <td>780.0</td>\n",
       "      <td>28</td>\n",
       "      <td>23.696601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>159.1</td>\n",
       "      <td>186.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>989.6</td>\n",
       "      <td>788.9</td>\n",
       "      <td>28</td>\n",
       "      <td>32.768036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>260.9</td>\n",
       "      <td>100.5</td>\n",
       "      <td>78.3</td>\n",
       "      <td>200.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>864.5</td>\n",
       "      <td>761.5</td>\n",
       "      <td>28</td>\n",
       "      <td>32.401235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cement (component 1)(kg in a m^3 mixture)  \\\n",
       "0                                         540.0   \n",
       "1                                         540.0   \n",
       "2                                         332.5   \n",
       "3                                         332.5   \n",
       "4                                         198.6   \n",
       "...                                         ...   \n",
       "1025                                      276.4   \n",
       "1026                                      322.2   \n",
       "1027                                      148.5   \n",
       "1028                                      159.1   \n",
       "1029                                      260.9   \n",
       "\n",
       "      Blast Furnace Slag (component 2)(kg in a m^3 mixture)  \\\n",
       "0                                                   0.0       \n",
       "1                                                   0.0       \n",
       "2                                                 142.5       \n",
       "3                                                 142.5       \n",
       "4                                                 132.4       \n",
       "...                                                 ...       \n",
       "1025                                              116.0       \n",
       "1026                                                0.0       \n",
       "1027                                              139.4       \n",
       "1028                                              186.7       \n",
       "1029                                              100.5       \n",
       "\n",
       "      Fly Ash (component 3)(kg in a m^3 mixture)  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "...                                          ...   \n",
       "1025                                        90.3   \n",
       "1026                                       115.6   \n",
       "1027                                       108.6   \n",
       "1028                                         0.0   \n",
       "1029                                        78.3   \n",
       "\n",
       "      Water  (component 4)(kg in a m^3 mixture)  \\\n",
       "0                                         162.0   \n",
       "1                                         162.0   \n",
       "2                                         228.0   \n",
       "3                                         228.0   \n",
       "4                                         192.0   \n",
       "...                                         ...   \n",
       "1025                                      179.6   \n",
       "1026                                      196.0   \n",
       "1027                                      192.7   \n",
       "1028                                      175.6   \n",
       "1029                                      200.6   \n",
       "\n",
       "      Superplasticizer (component 5)(kg in a m^3 mixture)  \\\n",
       "0                                                   2.5     \n",
       "1                                                   2.5     \n",
       "2                                                   0.0     \n",
       "3                                                   0.0     \n",
       "4                                                   0.0     \n",
       "...                                                 ...     \n",
       "1025                                                8.9     \n",
       "1026                                               10.4     \n",
       "1027                                                6.1     \n",
       "1028                                               11.3     \n",
       "1029                                                8.6     \n",
       "\n",
       "      Coarse Aggregate  (component 6)(kg in a m^3 mixture)  \\\n",
       "0                                                1040.0      \n",
       "1                                                1055.0      \n",
       "2                                                 932.0      \n",
       "3                                                 932.0      \n",
       "4                                                 978.4      \n",
       "...                                                 ...      \n",
       "1025                                              870.1      \n",
       "1026                                              817.9      \n",
       "1027                                              892.4      \n",
       "1028                                              989.6      \n",
       "1029                                              864.5      \n",
       "\n",
       "      Fine Aggregate (component 7)(kg in a m^3 mixture)  Age (day)  \\\n",
       "0                                                 676.0         28   \n",
       "1                                                 676.0         28   \n",
       "2                                                 594.0        270   \n",
       "3                                                 594.0        365   \n",
       "4                                                 825.5        360   \n",
       "...                                                 ...        ...   \n",
       "1025                                              768.3         28   \n",
       "1026                                              813.4         28   \n",
       "1027                                              780.0         28   \n",
       "1028                                              788.9         28   \n",
       "1029                                              761.5         28   \n",
       "\n",
       "      Concrete compressive strength(MPa, megapascals)   \n",
       "0                                            79.986111  \n",
       "1                                            61.887366  \n",
       "2                                            40.269535  \n",
       "3                                            41.052780  \n",
       "4                                            44.296075  \n",
       "...                                                ...  \n",
       "1025                                         44.284354  \n",
       "1026                                         31.178794  \n",
       "1027                                         23.696601  \n",
       "1028                                         32.768036  \n",
       "1029                                         32.401235  \n",
       "\n",
       "[1030 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df56bd6-574d-475a-993e-ee6e8364ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the corretlation matrix\n",
    "correlation_matrix = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514883b5-fa17-4fe7-b305-0a524681eb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement (component 1)(kg in a m^3 mixture)</th>\n",
       "      <th>Blast Furnace Slag (component 2)(kg in a m^3 mixture)</th>\n",
       "      <th>Fly Ash (component 3)(kg in a m^3 mixture)</th>\n",
       "      <th>Water  (component 4)(kg in a m^3 mixture)</th>\n",
       "      <th>Superplasticizer (component 5)(kg in a m^3 mixture)</th>\n",
       "      <th>Coarse Aggregate  (component 6)(kg in a m^3 mixture)</th>\n",
       "      <th>Fine Aggregate (component 7)(kg in a m^3 mixture)</th>\n",
       "      <th>Age (day)</th>\n",
       "      <th>Concrete compressive strength(MPa, megapascals)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cement (component 1)(kg in a m^3 mixture)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blast Furnace Slag (component 2)(kg in a m^3 mixture)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fly Ash (component 3)(kg in a m^3 mixture)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water  (component 4)(kg in a m^3 mixture)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Superplasticizer (component 5)(kg in a m^3 mixture)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coarse Aggregate  (component 6)(kg in a m^3 mixture)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine Aggregate (component 7)(kg in a m^3 mixture)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age (day)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Concrete compressive strength(MPa, megapascals)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Cement (component 1)(kg in a m^3 mixture)  \\\n",
       "Cement (component 1)(kg in a m^3 mixture)                                                 1.0   \n",
       "Blast Furnace Slag (component 2)(kg in a m^3 mi...                                        NaN   \n",
       "Fly Ash (component 3)(kg in a m^3 mixture)                                                NaN   \n",
       "Water  (component 4)(kg in a m^3 mixture)                                                 NaN   \n",
       "Superplasticizer (component 5)(kg in a m^3 mixt...                                        NaN   \n",
       "Coarse Aggregate  (component 6)(kg in a m^3 mix...                                        NaN   \n",
       "Fine Aggregate (component 7)(kg in a m^3 mixture)                                         NaN   \n",
       "Age (day)                                                                                 NaN   \n",
       "Concrete compressive strength(MPa, megapascals)                                           NaN   \n",
       "\n",
       "                                                    Blast Furnace Slag (component 2)(kg in a m^3 mixture)  \\\n",
       "Cement (component 1)(kg in a m^3 mixture)                                                         NaN       \n",
       "Blast Furnace Slag (component 2)(kg in a m^3 mi...                                                1.0       \n",
       "Fly Ash (component 3)(kg in a m^3 mixture)                                                        NaN       \n",
       "Water  (component 4)(kg in a m^3 mixture)                                                         NaN       \n",
       "Superplasticizer (component 5)(kg in a m^3 mixt...                                                NaN       \n",
       "Coarse Aggregate  (component 6)(kg in a m^3 mix...                                                NaN       \n",
       "Fine Aggregate (component 7)(kg in a m^3 mixture)                                                 NaN       \n",
       "Age (day)                                                                                         NaN       \n",
       "Concrete compressive strength(MPa, megapascals)                                                   NaN       \n",
       "\n",
       "                                                    Fly Ash (component 3)(kg in a m^3 mixture)  \\\n",
       "Cement (component 1)(kg in a m^3 mixture)                                                  NaN   \n",
       "Blast Furnace Slag (component 2)(kg in a m^3 mi...                                         NaN   \n",
       "Fly Ash (component 3)(kg in a m^3 mixture)                                                 1.0   \n",
       "Water  (component 4)(kg in a m^3 mixture)                                                  NaN   \n",
       "Superplasticizer (component 5)(kg in a m^3 mixt...                                         NaN   \n",
       "Coarse Aggregate  (component 6)(kg in a m^3 mix...                                         NaN   \n",
       "Fine Aggregate (component 7)(kg in a m^3 mixture)                                          NaN   \n",
       "Age (day)                                                                                  NaN   \n",
       "Concrete compressive strength(MPa, megapascals)                                            NaN   \n",
       "\n",
       "                                                    Water  (component 4)(kg in a m^3 mixture)  \\\n",
       "Cement (component 1)(kg in a m^3 mixture)                                                 NaN   \n",
       "Blast Furnace Slag (component 2)(kg in a m^3 mi...                                        NaN   \n",
       "Fly Ash (component 3)(kg in a m^3 mixture)                                                NaN   \n",
       "Water  (component 4)(kg in a m^3 mixture)                                                 1.0   \n",
       "Superplasticizer (component 5)(kg in a m^3 mixt...                                        NaN   \n",
       "Coarse Aggregate  (component 6)(kg in a m^3 mix...                                        NaN   \n",
       "Fine Aggregate (component 7)(kg in a m^3 mixture)                                         NaN   \n",
       "Age (day)                                                                                 NaN   \n",
       "Concrete compressive strength(MPa, megapascals)                                           NaN   \n",
       "\n",
       "                                                    Superplasticizer (component 5)(kg in a m^3 mixture)  \\\n",
       "Cement (component 1)(kg in a m^3 mixture)                                                         NaN     \n",
       "Blast Furnace Slag (component 2)(kg in a m^3 mi...                                                NaN     \n",
       "Fly Ash (component 3)(kg in a m^3 mixture)                                                        NaN     \n",
       "Water  (component 4)(kg in a m^3 mixture)                                                         NaN     \n",
       "Superplasticizer (component 5)(kg in a m^3 mixt...                                                1.0     \n",
       "Coarse Aggregate  (component 6)(kg in a m^3 mix...                                                NaN     \n",
       "Fine Aggregate (component 7)(kg in a m^3 mixture)                                                 NaN     \n",
       "Age (day)                                                                                         NaN     \n",
       "Concrete compressive strength(MPa, megapascals)                                                   NaN     \n",
       "\n",
       "                                                    Coarse Aggregate  (component 6)(kg in a m^3 mixture)  \\\n",
       "Cement (component 1)(kg in a m^3 mixture)                                                         NaN      \n",
       "Blast Furnace Slag (component 2)(kg in a m^3 mi...                                                NaN      \n",
       "Fly Ash (component 3)(kg in a m^3 mixture)                                                        NaN      \n",
       "Water  (component 4)(kg in a m^3 mixture)                                                         NaN      \n",
       "Superplasticizer (component 5)(kg in a m^3 mixt...                                                NaN      \n",
       "Coarse Aggregate  (component 6)(kg in a m^3 mix...                                                1.0      \n",
       "Fine Aggregate (component 7)(kg in a m^3 mixture)                                                 NaN      \n",
       "Age (day)                                                                                         NaN      \n",
       "Concrete compressive strength(MPa, megapascals)                                                   NaN      \n",
       "\n",
       "                                                    Fine Aggregate (component 7)(kg in a m^3 mixture)  \\\n",
       "Cement (component 1)(kg in a m^3 mixture)                                                         NaN   \n",
       "Blast Furnace Slag (component 2)(kg in a m^3 mi...                                                NaN   \n",
       "Fly Ash (component 3)(kg in a m^3 mixture)                                                        NaN   \n",
       "Water  (component 4)(kg in a m^3 mixture)                                                         NaN   \n",
       "Superplasticizer (component 5)(kg in a m^3 mixt...                                                NaN   \n",
       "Coarse Aggregate  (component 6)(kg in a m^3 mix...                                                NaN   \n",
       "Fine Aggregate (component 7)(kg in a m^3 mixture)                                                 1.0   \n",
       "Age (day)                                                                                         NaN   \n",
       "Concrete compressive strength(MPa, megapascals)                                                   NaN   \n",
       "\n",
       "                                                    Age (day)  \\\n",
       "Cement (component 1)(kg in a m^3 mixture)                 NaN   \n",
       "Blast Furnace Slag (component 2)(kg in a m^3 mi...        NaN   \n",
       "Fly Ash (component 3)(kg in a m^3 mixture)                NaN   \n",
       "Water  (component 4)(kg in a m^3 mixture)                 NaN   \n",
       "Superplasticizer (component 5)(kg in a m^3 mixt...        NaN   \n",
       "Coarse Aggregate  (component 6)(kg in a m^3 mix...        NaN   \n",
       "Fine Aggregate (component 7)(kg in a m^3 mixture)         NaN   \n",
       "Age (day)                                                 1.0   \n",
       "Concrete compressive strength(MPa, megapascals)           NaN   \n",
       "\n",
       "                                                    Concrete compressive strength(MPa, megapascals)   \n",
       "Cement (component 1)(kg in a m^3 mixture)                                                        NaN  \n",
       "Blast Furnace Slag (component 2)(kg in a m^3 mi...                                               NaN  \n",
       "Fly Ash (component 3)(kg in a m^3 mixture)                                                       NaN  \n",
       "Water  (component 4)(kg in a m^3 mixture)                                                        NaN  \n",
       "Superplasticizer (component 5)(kg in a m^3 mixt...                                               NaN  \n",
       "Coarse Aggregate  (component 6)(kg in a m^3 mix...                                               NaN  \n",
       "Fine Aggregate (component 7)(kg in a m^3 mixture)                                                NaN  \n",
       "Age (day)                                                                                        NaN  \n",
       "Concrete compressive strength(MPa, megapascals)                                                  1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there is no highly correlated columns or features \n",
    "correlation_matrix[correlation_matrix > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa9af96-237a-4367-b233-b5b200aaa60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataframe that doesnt include the target values in preparation for ML model\n",
    "df_without_label = df.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b02a4bb-e20e-4f50-b536-29bf9879967d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement (component 1)(kg in a m^3 mixture)</th>\n",
       "      <th>Blast Furnace Slag (component 2)(kg in a m^3 mixture)</th>\n",
       "      <th>Fly Ash (component 3)(kg in a m^3 mixture)</th>\n",
       "      <th>Water  (component 4)(kg in a m^3 mixture)</th>\n",
       "      <th>Superplasticizer (component 5)(kg in a m^3 mixture)</th>\n",
       "      <th>Coarse Aggregate  (component 6)(kg in a m^3 mixture)</th>\n",
       "      <th>Fine Aggregate (component 7)(kg in a m^3 mixture)</th>\n",
       "      <th>Age (day)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>276.4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>179.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>870.1</td>\n",
       "      <td>768.3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>322.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.6</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>817.9</td>\n",
       "      <td>813.4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>148.5</td>\n",
       "      <td>139.4</td>\n",
       "      <td>108.6</td>\n",
       "      <td>192.7</td>\n",
       "      <td>6.1</td>\n",
       "      <td>892.4</td>\n",
       "      <td>780.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>159.1</td>\n",
       "      <td>186.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>989.6</td>\n",
       "      <td>788.9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>260.9</td>\n",
       "      <td>100.5</td>\n",
       "      <td>78.3</td>\n",
       "      <td>200.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>864.5</td>\n",
       "      <td>761.5</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cement (component 1)(kg in a m^3 mixture)  \\\n",
       "0                                         540.0   \n",
       "1                                         540.0   \n",
       "2                                         332.5   \n",
       "3                                         332.5   \n",
       "4                                         198.6   \n",
       "...                                         ...   \n",
       "1025                                      276.4   \n",
       "1026                                      322.2   \n",
       "1027                                      148.5   \n",
       "1028                                      159.1   \n",
       "1029                                      260.9   \n",
       "\n",
       "      Blast Furnace Slag (component 2)(kg in a m^3 mixture)  \\\n",
       "0                                                   0.0       \n",
       "1                                                   0.0       \n",
       "2                                                 142.5       \n",
       "3                                                 142.5       \n",
       "4                                                 132.4       \n",
       "...                                                 ...       \n",
       "1025                                              116.0       \n",
       "1026                                                0.0       \n",
       "1027                                              139.4       \n",
       "1028                                              186.7       \n",
       "1029                                              100.5       \n",
       "\n",
       "      Fly Ash (component 3)(kg in a m^3 mixture)  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "...                                          ...   \n",
       "1025                                        90.3   \n",
       "1026                                       115.6   \n",
       "1027                                       108.6   \n",
       "1028                                         0.0   \n",
       "1029                                        78.3   \n",
       "\n",
       "      Water  (component 4)(kg in a m^3 mixture)  \\\n",
       "0                                         162.0   \n",
       "1                                         162.0   \n",
       "2                                         228.0   \n",
       "3                                         228.0   \n",
       "4                                         192.0   \n",
       "...                                         ...   \n",
       "1025                                      179.6   \n",
       "1026                                      196.0   \n",
       "1027                                      192.7   \n",
       "1028                                      175.6   \n",
       "1029                                      200.6   \n",
       "\n",
       "      Superplasticizer (component 5)(kg in a m^3 mixture)  \\\n",
       "0                                                   2.5     \n",
       "1                                                   2.5     \n",
       "2                                                   0.0     \n",
       "3                                                   0.0     \n",
       "4                                                   0.0     \n",
       "...                                                 ...     \n",
       "1025                                                8.9     \n",
       "1026                                               10.4     \n",
       "1027                                                6.1     \n",
       "1028                                               11.3     \n",
       "1029                                                8.6     \n",
       "\n",
       "      Coarse Aggregate  (component 6)(kg in a m^3 mixture)  \\\n",
       "0                                                1040.0      \n",
       "1                                                1055.0      \n",
       "2                                                 932.0      \n",
       "3                                                 932.0      \n",
       "4                                                 978.4      \n",
       "...                                                 ...      \n",
       "1025                                              870.1      \n",
       "1026                                              817.9      \n",
       "1027                                              892.4      \n",
       "1028                                              989.6      \n",
       "1029                                              864.5      \n",
       "\n",
       "      Fine Aggregate (component 7)(kg in a m^3 mixture)  Age (day)  \n",
       "0                                                 676.0         28  \n",
       "1                                                 676.0         28  \n",
       "2                                                 594.0        270  \n",
       "3                                                 594.0        365  \n",
       "4                                                 825.5        360  \n",
       "...                                                 ...        ...  \n",
       "1025                                              768.3         28  \n",
       "1026                                              813.4         28  \n",
       "1027                                              780.0         28  \n",
       "1028                                              788.9         28  \n",
       "1029                                              761.5         28  \n",
       "\n",
       "[1030 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making sure the target values were omitted properly\n",
    "df_without_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecbcc058-b5ab-4bd0-89a7-b6a6e0b23edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the pipeline for scaling the data through the sklearn libraries\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#https://scikit-learn.org/1.5/modules/preprocessing.html\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scalingAndVectorizer', StandardScaler(), df_without_label.columns.tolist())\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e41ffaa-a741-4233-abdb-d0e993a738d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the dataframe without label to the pipeline and transforming\n",
    "transformed_data = preprocessor.fit_transform(df_without_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c214020f-73f8-4aa4-9ee2-c0cabf130a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b33b9050-3b8d-4479-8786-0e0e9e764771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the target values only from the main dataframe for the training\n",
    "df_label = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50e2519b-4723-43ef-ba19-3e041bc90afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklean expects numpy array for the labels, thus converting the target value pandas dataframe to numpy\n",
    "label = df_label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d75fe152-b74a-443e-bc4b-5b881cd693a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_data, label, test_size=0.3, random_state=42)\n",
    "#https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "63df54b1-b75e-42e9-b25f-22bb05ff6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#defining function to turn numpy to Torch\n",
    "def pandas_to_torch(x, y):\n",
    "    x_flt = x.astype(np.float32)\n",
    "    y_flt = y.astype(np.float32)    \n",
    "    x = torch.from_numpy(x_flt)\n",
    "    y = torch.from_numpy(y_flt)\n",
    "\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "#converting Pandas df to Torch\n",
    "x_train, y_train = pandas_to_torch(X_train, y_train)\n",
    "x_test, y_test = pandas_to_torch (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "523f357e-1317-4594-af11-a376ddb25fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8280, -0.8569,  0.7243,  ...,  0.4308,  1.6512, -0.2797],\n",
       "        [ 0.4196, -0.8569, -0.8471,  ...,  1.1591,  0.1176, -0.6123],\n",
       "        [ 0.3603,  1.6072, -0.8471,  ..., -1.5546,  1.3330, -0.6757],\n",
       "        ...,\n",
       "        [-0.8695, -0.8569,  1.1099,  ...,  1.3650,  0.3160,  0.8606],\n",
       "        [ 1.8556,  0.5207, -0.8471,  ..., -1.5546,  0.0988, -0.2797],\n",
       "        [ 0.3143, -0.8569,  0.9194,  ..., -0.6166,  0.1176, -0.2797]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "861a43b8-adad-46aa-8d14-03c7c9917102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING CUSTOM DATALOAD AND DATASET\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyDataset(Dataset): \n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx],self.y[idx])\n",
    "\n",
    "training_set = MyDataset(x_train, y_train)\n",
    "testing_set = MyDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "110fef99-fc22-4594-a21d-8cba6cc40c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available!\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# USING MPS ACCELARATION FOR FASTER PROCESSING\n",
    "###\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS backend is available!\")\n",
    "else:\n",
    "    print(\"MPS backend is not available.\")\n",
    "\n",
    "# Using the MPS acceleration\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce54fb-ad8e-4c80-add1-5424c30c0f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([721, 8])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the dimensions of the trainnig and testing sets\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5a73e42a-831a-48bf-bfe4-2e18992f9eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([721])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2988c484-65b5-4f0f-a233-427ebce4318f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([309, 8])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9a7b1840-9a18-4845-ad46-63fc390f85f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([309])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b264da-8ec0-4f6c-ab61-c0f08d56d380",
   "metadata": {},
   "source": [
    "# Setting up PyTorch NN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "17d8000e-a00d-46ca-aa86-0770987f5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING THE CLASS FOR THE MLP MODEL\n",
    "from torch import nn\n",
    "class myMultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(  # here we stack multiple layers together\n",
    "            nn.Linear(input_dim,16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8,4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4,output_dim)   \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y = self.sequential(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "35c6c5fd-3bbb-4948-abf7-3373294621a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myMultiLayerPerceptron(\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=4, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mymodel_hyper = myMultiLayerPerceptron(8, 1) # creating a model instance with input dimension 31 and output dimension 1\n",
    "mymodel_hyper.to(device)\n",
    "print(mymodel_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15145b4a-d911-4971-83f7-e9ce2b0d4045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 0, train_loss=1567.29345703125\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 1, train_loss=1559.1258544921875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 2, train_loss=1554.64892578125\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 3, train_loss=1571.169677734375\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 4, train_loss=1531.8778076171875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 5, train_loss=1522.74755859375\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 6, train_loss=1513.6478271484375\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 7, train_loss=1460.6160888671875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 8, train_loss=1415.2674560546875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 9, train_loss=1323.9061279296875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 10, train_loss=1218.2127685546875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 11, train_loss=1095.7421875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 12, train_loss=925.639404296875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 13, train_loss=766.4039916992188\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 14, train_loss=596.333984375\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 15, train_loss=456.87847900390625\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 16, train_loss=348.2648010253906\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 17, train_loss=275.79119873046875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 18, train_loss=225.49119567871094\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0001 and batch_size=8\n",
      ", Epoch = 19, train_loss=197.16867065429688\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 0, train_loss=1570.1705322265625\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 1, train_loss=1555.794189453125\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 2, train_loss=1573.6015625\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 3, train_loss=1554.83251953125\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 4, train_loss=1557.5855712890625\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 5, train_loss=1541.7618408203125\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 6, train_loss=1525.6611328125\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 7, train_loss=1576.126220703125\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 8, train_loss=1539.1898193359375\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 9, train_loss=1514.306884765625\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 10, train_loss=1577.406982421875\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 11, train_loss=1543.094482421875\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 12, train_loss=1487.8974609375\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 13, train_loss=1508.885986328125\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 14, train_loss=1535.432373046875\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 15, train_loss=1446.33935546875\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 16, train_loss=1359.0179443359375\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 17, train_loss=1332.759033203125\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 18, train_loss=1255.456787109375\n",
      "\n",
      "For lr=0.0001 and batch_size=16\n",
      ", Epoch = 19, train_loss=1276.6817626953125\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 0, train_loss=1577.7113037109375\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 1, train_loss=1585.542236328125\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 2, train_loss=1582.944091796875\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 3, train_loss=1579.231201171875\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 4, train_loss=1572.5164794921875\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 5, train_loss=1588.6431884765625\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 6, train_loss=1576.2763671875\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 7, train_loss=1589.5234375\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 8, train_loss=1570.3021240234375\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 9, train_loss=1571.3792724609375\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 10, train_loss=1567.81591796875\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 11, train_loss=1570.901123046875\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 12, train_loss=1579.4886474609375\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 13, train_loss=1565.0777587890625\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 14, train_loss=1563.7642822265625\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 15, train_loss=1578.823486328125\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 16, train_loss=1572.832763671875\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 17, train_loss=1577.9561767578125\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 18, train_loss=1567.593017578125\n",
      "\n",
      "For lr=0.0001 and batch_size=32\n",
      ", Epoch = 19, train_loss=1572.5999755859375\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 0, train_loss=1613.9622802734375\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 1, train_loss=1596.2584228515625\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 2, train_loss=1590.7867431640625\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 3, train_loss=1571.7301025390625\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 4, train_loss=1647.00732421875\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 5, train_loss=1608.0128173828125\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 6, train_loss=1583.7381591796875\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 7, train_loss=1598.0159912109375\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 8, train_loss=1577.6412353515625\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 9, train_loss=1564.7371826171875\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 10, train_loss=1615.3876953125\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 11, train_loss=1614.3590087890625\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 12, train_loss=1585.3087158203125\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 13, train_loss=1592.9637451171875\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 14, train_loss=1581.65966796875\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 15, train_loss=1612.5020751953125\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 16, train_loss=1607.8516845703125\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 17, train_loss=1573.7020263671875\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 18, train_loss=1591.3131103515625\n",
      "\n",
      "For lr=0.0001 and batch_size=64\n",
      ", Epoch = 19, train_loss=1615.6158447265625\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 0, train_loss=1583.4462890625\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 1, train_loss=1579.3250732421875\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 2, train_loss=1588.5281982421875\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 3, train_loss=1586.76708984375\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 4, train_loss=1577.9874267578125\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 5, train_loss=1572.18115234375\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 6, train_loss=1572.3204345703125\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 7, train_loss=1579.4212646484375\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 8, train_loss=1571.40673828125\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 9, train_loss=1576.1802978515625\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 10, train_loss=1582.7557373046875\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 11, train_loss=1576.4129638671875\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 12, train_loss=1577.5650634765625\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 13, train_loss=1564.3958740234375\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 14, train_loss=1560.4156494140625\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 15, train_loss=1590.8858642578125\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 16, train_loss=1585.32080078125\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 17, train_loss=1576.4683837890625\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 18, train_loss=1576.3885498046875\n",
      "\n",
      "For lr=0.0001 and batch_size=128\n",
      ", Epoch = 19, train_loss=1574.1800537109375\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 0, train_loss=1560.6832275390625\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 1, train_loss=1598.974365234375\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 2, train_loss=1551.6795654296875\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 3, train_loss=1571.188720703125\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 4, train_loss=1538.5611572265625\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 5, train_loss=1550.3538818359375\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 6, train_loss=1542.0404052734375\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 7, train_loss=1529.542724609375\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 8, train_loss=1530.9595947265625\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 9, train_loss=1530.2393798828125\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 10, train_loss=1521.8648681640625\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 11, train_loss=1520.882568359375\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 12, train_loss=1535.9217529296875\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 13, train_loss=1547.2855224609375\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 14, train_loss=1565.4625244140625\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 15, train_loss=1515.66162109375\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 16, train_loss=1508.0535888671875\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 17, train_loss=1514.885986328125\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 18, train_loss=1496.279052734375\n",
      "\n",
      "For lr=0.0005 and batch_size=8\n",
      ", Epoch = 19, train_loss=1542.2568359375\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 0, train_loss=1557.625\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 1, train_loss=1525.43701171875\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 2, train_loss=1519.67626953125\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 3, train_loss=1505.6070556640625\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 4, train_loss=1380.0596923828125\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 5, train_loss=1179.330078125\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 6, train_loss=823.74658203125\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 7, train_loss=433.9038391113281\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 8, train_loss=268.74981689453125\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 9, train_loss=213.27072143554688\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 10, train_loss=194.7008514404297\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 11, train_loss=185.55332946777344\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 12, train_loss=182.01211547851562\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 13, train_loss=176.07681274414062\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 14, train_loss=182.58863830566406\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 15, train_loss=171.5558319091797\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 16, train_loss=169.12850952148438\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 17, train_loss=166.90428161621094\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 18, train_loss=163.36172485351562\n",
      "\n",
      "For lr=0.0005 and batch_size=16\n",
      ", Epoch = 19, train_loss=166.98609924316406\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 0, train_loss=1590.2664794921875\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 1, train_loss=1581.0198974609375\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 2, train_loss=1592.6031494140625\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 3, train_loss=1590.96435546875\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 4, train_loss=1574.4847412109375\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 5, train_loss=1580.5726318359375\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 6, train_loss=1574.0869140625\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 7, train_loss=1570.313720703125\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 8, train_loss=1548.8792724609375\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 9, train_loss=1520.2252197265625\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 10, train_loss=1477.1046142578125\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 11, train_loss=1379.6005859375\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 12, train_loss=1258.325439453125\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 13, train_loss=1078.6478271484375\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 14, train_loss=852.3888549804688\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 15, train_loss=625.32763671875\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 16, train_loss=412.6435852050781\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 17, train_loss=278.5437316894531\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 18, train_loss=219.0628204345703\n",
      "\n",
      "For lr=0.0005 and batch_size=32\n",
      ", Epoch = 19, train_loss=202.8527374267578\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 0, train_loss=1588.5279541015625\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 1, train_loss=1600.8802490234375\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 2, train_loss=1538.8306884765625\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 3, train_loss=1566.7149658203125\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 4, train_loss=1565.2744140625\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 5, train_loss=1574.5189208984375\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 6, train_loss=1553.833984375\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 7, train_loss=1562.33544921875\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 8, train_loss=1564.8734130859375\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 9, train_loss=1501.1011962890625\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 10, train_loss=1532.4246826171875\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 11, train_loss=1563.2645263671875\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 12, train_loss=1497.1207275390625\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 13, train_loss=1501.9544677734375\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 14, train_loss=1390.9737548828125\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 15, train_loss=1360.625732421875\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 16, train_loss=1283.09375\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 17, train_loss=1199.6614990234375\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 18, train_loss=1130.7052001953125\n",
      "\n",
      "For lr=0.0005 and batch_size=64\n",
      ", Epoch = 19, train_loss=1027.7725830078125\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 0, train_loss=1617.6624755859375\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 1, train_loss=1611.1201171875\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 2, train_loss=1622.876953125\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 3, train_loss=1627.2301025390625\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 4, train_loss=1622.48291015625\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 5, train_loss=1599.4970703125\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 6, train_loss=1623.765625\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 7, train_loss=1611.03125\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 8, train_loss=1609.24072265625\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 9, train_loss=1609.2178955078125\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 10, train_loss=1595.0762939453125\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 11, train_loss=1602.4493408203125\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 12, train_loss=1594.4268798828125\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 13, train_loss=1584.93896484375\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 14, train_loss=1576.0445556640625\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 15, train_loss=1569.9605712890625\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 16, train_loss=1560.8037109375\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 17, train_loss=1555.65380859375\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 18, train_loss=1549.2718505859375\n",
      "\n",
      "For lr=0.0005 and batch_size=128\n",
      ", Epoch = 19, train_loss=1537.0701904296875\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 0, train_loss=1600.5186767578125\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 1, train_loss=1271.1962890625\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 2, train_loss=254.74302673339844\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 3, train_loss=173.48727416992188\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 4, train_loss=161.397705078125\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 5, train_loss=148.76258850097656\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 6, train_loss=141.5199432373047\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 7, train_loss=138.1042938232422\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 8, train_loss=133.97039794921875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 9, train_loss=130.95350646972656\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 10, train_loss=129.34767150878906\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 11, train_loss=126.5537338256836\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 12, train_loss=124.13592529296875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 13, train_loss=121.12422943115234\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 14, train_loss=120.311767578125\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 15, train_loss=116.92623138427734\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 16, train_loss=116.1783218383789\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 17, train_loss=116.49508666992188\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 18, train_loss=112.22764587402344\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.001 and batch_size=8\n",
      ", Epoch = 19, train_loss=110.05418395996094\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 0, train_loss=1551.9696044921875\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 1, train_loss=1454.91796875\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 2, train_loss=1040.1251220703125\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 3, train_loss=335.5334777832031\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 4, train_loss=192.54872131347656\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 5, train_loss=176.5644073486328\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 6, train_loss=165.74168395996094\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 7, train_loss=158.73538208007812\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 8, train_loss=155.487060546875\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 9, train_loss=148.37840270996094\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 10, train_loss=147.04110717773438\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 11, train_loss=139.67477416992188\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 12, train_loss=141.42929077148438\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 13, train_loss=141.06663513183594\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 14, train_loss=132.3627471923828\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 15, train_loss=137.49293518066406\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 16, train_loss=128.9005126953125\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 17, train_loss=152.00987243652344\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 18, train_loss=132.06321716308594\n",
      "\n",
      "For lr=0.001 and batch_size=16\n",
      ", Epoch = 19, train_loss=129.44508361816406\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 0, train_loss=1598.403076171875\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 1, train_loss=1605.825439453125\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 2, train_loss=1596.9952392578125\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 3, train_loss=1606.6124267578125\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 4, train_loss=1588.3656005859375\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 5, train_loss=1601.466064453125\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 6, train_loss=1578.2803955078125\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 7, train_loss=1572.4429931640625\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 8, train_loss=1577.9886474609375\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 9, train_loss=1578.0369873046875\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 10, train_loss=1578.239501953125\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 11, train_loss=1578.4398193359375\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 12, train_loss=1572.857177734375\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 13, train_loss=1571.5040283203125\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 14, train_loss=1564.91015625\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 15, train_loss=1561.3232421875\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 16, train_loss=1551.6776123046875\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 17, train_loss=1559.7626953125\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 18, train_loss=1548.4921875\n",
      "\n",
      "For lr=0.001 and batch_size=32\n",
      ", Epoch = 19, train_loss=1553.0001220703125\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 0, train_loss=1609.7462158203125\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 1, train_loss=1607.9971923828125\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 2, train_loss=1591.78759765625\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 3, train_loss=1567.3272705078125\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 4, train_loss=1565.82666015625\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 5, train_loss=1536.6234130859375\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 6, train_loss=1486.419921875\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 7, train_loss=1363.4803466796875\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 8, train_loss=1271.0218505859375\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 9, train_loss=1075.7945556640625\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 10, train_loss=861.744140625\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 11, train_loss=625.3232421875\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 12, train_loss=418.5665283203125\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 13, train_loss=323.228271484375\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 14, train_loss=275.8963317871094\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 15, train_loss=255.65069580078125\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 16, train_loss=228.6558837890625\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 17, train_loss=217.4322967529297\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 18, train_loss=201.31666564941406\n",
      "\n",
      "For lr=0.001 and batch_size=64\n",
      ", Epoch = 19, train_loss=205.8360595703125\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 0, train_loss=1580.1031494140625\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 1, train_loss=1601.2958984375\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 2, train_loss=1583.00146484375\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 3, train_loss=1583.06494140625\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 4, train_loss=1581.9747314453125\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 5, train_loss=1562.5826416015625\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 6, train_loss=1568.8492431640625\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 7, train_loss=1552.5191650390625\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 8, train_loss=1569.6314697265625\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 9, train_loss=1556.9404296875\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 10, train_loss=1561.1812744140625\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 11, train_loss=1567.5225830078125\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 12, train_loss=1575.2191162109375\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 13, train_loss=1570.2730712890625\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 14, train_loss=1585.6341552734375\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 15, train_loss=1566.20166015625\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 16, train_loss=1580.3885498046875\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 17, train_loss=1570.3236083984375\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 18, train_loss=1578.0447998046875\n",
      "\n",
      "For lr=0.001 and batch_size=128\n",
      ", Epoch = 19, train_loss=1568.0362548828125\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 0, train_loss=785.0010986328125\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 1, train_loss=158.1378631591797\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 2, train_loss=143.4822540283203\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 3, train_loss=126.73905181884766\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 4, train_loss=126.06226348876953\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 5, train_loss=117.85482788085938\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 6, train_loss=119.32857513427734\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 7, train_loss=111.17649841308594\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 8, train_loss=91.70014190673828\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 9, train_loss=84.74644470214844\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 10, train_loss=75.0219955444336\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 11, train_loss=64.24729919433594\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 12, train_loss=59.60970687866211\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 13, train_loss=54.29462814331055\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 14, train_loss=50.00082015991211\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 15, train_loss=51.39136505126953\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 16, train_loss=51.08863067626953\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 17, train_loss=45.02674865722656\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 18, train_loss=45.627838134765625\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.005 and batch_size=8\n",
      ", Epoch = 19, train_loss=43.37458419799805\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 0, train_loss=1556.921875\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 1, train_loss=949.5632934570312\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 2, train_loss=194.91993713378906\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 3, train_loss=178.3577880859375\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 4, train_loss=148.71670532226562\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 5, train_loss=134.99180603027344\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 6, train_loss=132.4702606201172\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 7, train_loss=125.53697204589844\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 8, train_loss=117.70486450195312\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 9, train_loss=113.65074920654297\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 10, train_loss=117.9449691772461\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 11, train_loss=117.09632110595703\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 12, train_loss=115.52861022949219\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 13, train_loss=106.34880065917969\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 14, train_loss=110.1141128540039\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 15, train_loss=105.91072082519531\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 16, train_loss=106.98478698730469\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 17, train_loss=99.73738098144531\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 18, train_loss=98.92819213867188\n",
      "\n",
      "For lr=0.005 and batch_size=16\n",
      ", Epoch = 19, train_loss=97.47886657714844\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 0, train_loss=1553.2747802734375\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 1, train_loss=1301.7247314453125\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 2, train_loss=370.27813720703125\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 3, train_loss=176.1334228515625\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 4, train_loss=159.70225524902344\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 5, train_loss=145.45164489746094\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 6, train_loss=139.24215698242188\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 7, train_loss=134.63551330566406\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 8, train_loss=132.2559051513672\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 9, train_loss=131.2691650390625\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 10, train_loss=126.29267120361328\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 11, train_loss=124.44233703613281\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 12, train_loss=122.0934829711914\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 13, train_loss=117.77159118652344\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 14, train_loss=117.80863952636719\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 15, train_loss=113.820068359375\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 16, train_loss=112.38580322265625\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 17, train_loss=112.97254943847656\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 18, train_loss=115.24134826660156\n",
      "\n",
      "For lr=0.005 and batch_size=32\n",
      ", Epoch = 19, train_loss=110.17884826660156\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 0, train_loss=1606.1865234375\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 1, train_loss=1608.9852294921875\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 2, train_loss=1496.2445068359375\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 3, train_loss=1220.4029541015625\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 4, train_loss=529.9494018554688\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 5, train_loss=281.72100830078125\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 6, train_loss=203.5248565673828\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 7, train_loss=180.28111267089844\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 8, train_loss=164.3451385498047\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 9, train_loss=159.39329528808594\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 10, train_loss=157.9954376220703\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 11, train_loss=146.98524475097656\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 12, train_loss=147.7397003173828\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 13, train_loss=143.2069549560547\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 14, train_loss=138.5367889404297\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 15, train_loss=135.4223175048828\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 16, train_loss=129.67393493652344\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 17, train_loss=129.85671997070312\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 18, train_loss=126.42951202392578\n",
      "\n",
      "For lr=0.005 and batch_size=64\n",
      ", Epoch = 19, train_loss=124.10747528076172\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 0, train_loss=1585.1060791015625\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 1, train_loss=1580.0743408203125\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 2, train_loss=1578.9840087890625\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 3, train_loss=1577.9071044921875\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 4, train_loss=1577.4078369140625\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 5, train_loss=1569.2645263671875\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 6, train_loss=1573.6683349609375\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 7, train_loss=1560.80078125\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 8, train_loss=1575.9866943359375\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 9, train_loss=1568.61962890625\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 10, train_loss=1596.5474853515625\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 11, train_loss=1564.27099609375\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 12, train_loss=1568.4344482421875\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 13, train_loss=1554.9033203125\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 14, train_loss=1559.1871337890625\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 15, train_loss=1555.6265869140625\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 16, train_loss=1559.302734375\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 17, train_loss=1549.7337646484375\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 18, train_loss=1546.6669921875\n",
      "\n",
      "For lr=0.005 and batch_size=128\n",
      ", Epoch = 19, train_loss=1530.1107177734375\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 0, train_loss=533.0714721679688\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 1, train_loss=149.9406280517578\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 2, train_loss=126.79486846923828\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 3, train_loss=118.57916259765625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 4, train_loss=112.72772979736328\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 5, train_loss=103.43070983886719\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 6, train_loss=96.63551330566406\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 7, train_loss=96.78571319580078\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 8, train_loss=90.84851837158203\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 9, train_loss=84.53167724609375\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 10, train_loss=74.05352020263672\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 11, train_loss=63.1876335144043\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 12, train_loss=57.712284088134766\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 13, train_loss=58.00825119018555\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 14, train_loss=53.218807220458984\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 15, train_loss=50.80161666870117\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 16, train_loss=51.21052932739258\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 17, train_loss=40.65120315551758\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 18, train_loss=44.847774505615234\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 19, train_loss=41.71581268310547\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 0, train_loss=944.2118530273438\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 1, train_loss=180.8482666015625\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 2, train_loss=135.3932342529297\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 3, train_loss=130.16488647460938\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 4, train_loss=133.1368865966797\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 5, train_loss=134.47508239746094\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 6, train_loss=124.56710815429688\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 7, train_loss=127.04915618896484\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 8, train_loss=108.56475067138672\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 9, train_loss=109.40717315673828\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 10, train_loss=106.380859375\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 11, train_loss=110.2413558959961\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 12, train_loss=111.708251953125\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 13, train_loss=103.79085540771484\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 14, train_loss=75.96809387207031\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 15, train_loss=65.81634521484375\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 16, train_loss=61.758174896240234\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 17, train_loss=51.176979064941406\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 18, train_loss=46.9393310546875\n",
      "\n",
      "For lr=0.01 and batch_size=16\n",
      ", Epoch = 19, train_loss=47.444435119628906\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 0, train_loss=1519.083251953125\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 1, train_loss=445.7772216796875\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 2, train_loss=174.0344696044922\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 3, train_loss=150.9029541015625\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 4, train_loss=133.42144775390625\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 5, train_loss=127.87139892578125\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 6, train_loss=124.28247833251953\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 7, train_loss=120.40174102783203\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 8, train_loss=118.77520751953125\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 9, train_loss=118.6789779663086\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 10, train_loss=123.4081802368164\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 11, train_loss=111.70592498779297\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 12, train_loss=110.00711059570312\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 13, train_loss=109.66036987304688\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 14, train_loss=111.34037780761719\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 15, train_loss=107.25016021728516\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 16, train_loss=105.52389526367188\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 17, train_loss=104.6797103881836\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 18, train_loss=90.9354019165039\n",
      "\n",
      "For lr=0.01 and batch_size=32\n",
      ", Epoch = 19, train_loss=87.14457702636719\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 0, train_loss=1541.8812255859375\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 1, train_loss=1331.1929931640625\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 2, train_loss=459.4447937011719\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 3, train_loss=221.9487762451172\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 4, train_loss=182.834716796875\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 5, train_loss=159.50465393066406\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 6, train_loss=146.2301788330078\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 7, train_loss=135.84439086914062\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 8, train_loss=130.9630889892578\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 9, train_loss=126.96163940429688\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 10, train_loss=124.2376937866211\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 11, train_loss=126.44454956054688\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 12, train_loss=129.7483673095703\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 13, train_loss=119.38996124267578\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 14, train_loss=118.22747039794922\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 15, train_loss=114.5531234741211\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 16, train_loss=110.6234359741211\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 17, train_loss=105.43933868408203\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 18, train_loss=104.34227752685547\n",
      "\n",
      "For lr=0.01 and batch_size=64\n",
      ", Epoch = 19, train_loss=102.2388916015625\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 0, train_loss=1594.1385498046875\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 1, train_loss=1596.6728515625\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 2, train_loss=1568.1639404296875\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 3, train_loss=1447.5587158203125\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 4, train_loss=1034.2581787109375\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 5, train_loss=421.097900390625\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 6, train_loss=360.1769104003906\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 7, train_loss=221.54396057128906\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 8, train_loss=232.91575622558594\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 9, train_loss=196.1041259765625\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 10, train_loss=186.99668884277344\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 11, train_loss=169.2422637939453\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 12, train_loss=159.52842712402344\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 13, train_loss=157.35287475585938\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 14, train_loss=151.88096618652344\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 15, train_loss=145.41641235351562\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 16, train_loss=145.7589569091797\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 17, train_loss=138.48989868164062\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 18, train_loss=137.03594970703125\n",
      "\n",
      "For lr=0.01 and batch_size=128\n",
      ", Epoch = 19, train_loss=134.484375\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 0, train_loss=380.11236572265625\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 1, train_loss=155.34783935546875\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 2, train_loss=147.82484436035156\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 3, train_loss=129.67481994628906\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 4, train_loss=85.10909271240234\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 5, train_loss=83.79314422607422\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 6, train_loss=71.18651580810547\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 7, train_loss=56.80435562133789\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 8, train_loss=64.0213851928711\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 9, train_loss=49.96442413330078\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 10, train_loss=60.06967544555664\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 11, train_loss=70.55278778076172\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 12, train_loss=48.41773986816406\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 13, train_loss=49.19764709472656\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 14, train_loss=55.3007698059082\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 15, train_loss=54.55294418334961\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 16, train_loss=49.01398468017578\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 17, train_loss=47.14295196533203\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 18, train_loss=48.938507080078125\n",
      "\n",
      "For lr=0.05 and batch_size=8\n",
      ", Epoch = 19, train_loss=46.62587356567383\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 0, train_loss=446.1575622558594\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 1, train_loss=152.98297119140625\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 2, train_loss=118.96056365966797\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 3, train_loss=106.82806396484375\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 4, train_loss=79.85700225830078\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 5, train_loss=90.43897247314453\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 6, train_loss=90.35850524902344\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 7, train_loss=106.86927032470703\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 8, train_loss=79.35723114013672\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 9, train_loss=73.94692993164062\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 10, train_loss=78.21205139160156\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 11, train_loss=74.01354217529297\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 12, train_loss=74.44474029541016\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 13, train_loss=99.17188262939453\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 14, train_loss=75.93988800048828\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 15, train_loss=75.60194396972656\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 16, train_loss=55.42240524291992\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 17, train_loss=66.0045166015625\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 18, train_loss=100.22230529785156\n",
      "\n",
      "For lr=0.05 and batch_size=16\n",
      ", Epoch = 19, train_loss=67.89025115966797\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 0, train_loss=704.8521118164062\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 1, train_loss=214.87261962890625\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 2, train_loss=156.49331665039062\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 3, train_loss=128.08616638183594\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 4, train_loss=122.40301513671875\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 5, train_loss=131.6056671142578\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 6, train_loss=144.41165161132812\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 7, train_loss=120.31620788574219\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 8, train_loss=123.61860656738281\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 9, train_loss=125.24732208251953\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 10, train_loss=111.2109375\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 11, train_loss=114.65880584716797\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 12, train_loss=113.65876770019531\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 13, train_loss=118.880859375\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 14, train_loss=118.75736999511719\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 15, train_loss=111.298828125\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 16, train_loss=107.90668487548828\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 17, train_loss=102.58628845214844\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 18, train_loss=93.54666137695312\n",
      "\n",
      "For lr=0.05 and batch_size=32\n",
      ", Epoch = 19, train_loss=85.40184020996094\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 0, train_loss=972.2066040039062\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 1, train_loss=308.5999450683594\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 2, train_loss=204.11859130859375\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 3, train_loss=181.32591247558594\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 4, train_loss=144.8572235107422\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 5, train_loss=130.2227020263672\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 6, train_loss=125.4391860961914\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 7, train_loss=119.19754028320312\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 8, train_loss=124.49880981445312\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 9, train_loss=117.48796844482422\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 10, train_loss=110.98011016845703\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 11, train_loss=107.93411254882812\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 12, train_loss=103.72869873046875\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 13, train_loss=107.4678726196289\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 14, train_loss=101.17144012451172\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 15, train_loss=91.27149200439453\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 16, train_loss=88.59728240966797\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 17, train_loss=72.2956771850586\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 18, train_loss=68.39408874511719\n",
      "\n",
      "For lr=0.05 and batch_size=64\n",
      ", Epoch = 19, train_loss=68.39221954345703\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 0, train_loss=1348.9044189453125\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 1, train_loss=397.848876953125\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 2, train_loss=294.3815612792969\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 3, train_loss=225.82640075683594\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 4, train_loss=180.90252685546875\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 5, train_loss=147.0100860595703\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 6, train_loss=151.13662719726562\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 7, train_loss=144.91546630859375\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 8, train_loss=131.71836853027344\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 9, train_loss=126.52179718017578\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 10, train_loss=124.67971801757812\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 11, train_loss=118.99755859375\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 12, train_loss=117.85160064697266\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 13, train_loss=117.10006713867188\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 14, train_loss=113.17404174804688\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 15, train_loss=111.11458587646484\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 16, train_loss=108.8650131225586\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 17, train_loss=107.71343231201172\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 18, train_loss=106.03158569335938\n",
      "\n",
      "For lr=0.05 and batch_size=128\n",
      ", Epoch = 19, train_loss=104.73746490478516\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 0, train_loss=421.2578430175781\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 1, train_loss=204.7199249267578\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 2, train_loss=159.38925170898438\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 3, train_loss=169.69610595703125\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 4, train_loss=108.669189453125\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 5, train_loss=98.88231658935547\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 6, train_loss=85.19327545166016\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 7, train_loss=320.4598388671875\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 8, train_loss=124.12638092041016\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 9, train_loss=93.98812866210938\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 10, train_loss=79.00100708007812\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 11, train_loss=74.8677749633789\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 12, train_loss=75.98997497558594\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 13, train_loss=84.49220275878906\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 14, train_loss=74.68712615966797\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 15, train_loss=89.1314468383789\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 16, train_loss=71.73571014404297\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 17, train_loss=68.92135620117188\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 18, train_loss=73.09583282470703\n",
      "\n",
      "For lr=0.1 and batch_size=8\n",
      ", Epoch = 19, train_loss=75.40030670166016\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 0, train_loss=1453.3685302734375\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 1, train_loss=1186.1185302734375\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 2, train_loss=948.31689453125\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 3, train_loss=773.0813598632812\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 4, train_loss=624.6116943359375\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 5, train_loss=524.8207397460938\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 6, train_loss=459.1092529296875\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 7, train_loss=393.75054931640625\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 8, train_loss=364.7102966308594\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 9, train_loss=348.74627685546875\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 10, train_loss=330.2106018066406\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 11, train_loss=292.3125305175781\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 12, train_loss=298.7511291503906\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 13, train_loss=283.8990478515625\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 14, train_loss=280.1674499511719\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 15, train_loss=278.50054931640625\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 16, train_loss=277.5298767089844\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 17, train_loss=283.39715576171875\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 18, train_loss=276.5957336425781\n",
      "\n",
      "For lr=0.1 and batch_size=16\n",
      ", Epoch = 19, train_loss=277.6822204589844\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 0, train_loss=586.6962280273438\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 1, train_loss=182.98707580566406\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 2, train_loss=152.5343780517578\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 3, train_loss=126.08793640136719\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 4, train_loss=125.45933532714844\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 5, train_loss=117.04839324951172\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 6, train_loss=84.3563003540039\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 7, train_loss=76.47620391845703\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 8, train_loss=76.48786926269531\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 9, train_loss=67.15397644042969\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 10, train_loss=80.69465637207031\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 11, train_loss=78.8931884765625\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 12, train_loss=82.68486022949219\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 13, train_loss=93.10200500488281\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 14, train_loss=59.3531379699707\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 15, train_loss=61.59186553955078\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 16, train_loss=56.985233306884766\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 17, train_loss=55.38396453857422\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 18, train_loss=59.073429107666016\n",
      "\n",
      "For lr=0.1 and batch_size=32\n",
      ", Epoch = 19, train_loss=76.82954406738281\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 0, train_loss=968.3466186523438\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 1, train_loss=424.386962890625\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 2, train_loss=193.26002502441406\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 3, train_loss=145.5257110595703\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 4, train_loss=133.17933654785156\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 5, train_loss=113.47606658935547\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 6, train_loss=96.75164794921875\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 7, train_loss=86.47977447509766\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 8, train_loss=65.98250579833984\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 9, train_loss=64.682373046875\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 10, train_loss=63.72639846801758\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 11, train_loss=59.99444580078125\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 12, train_loss=62.47710037231445\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 13, train_loss=58.47547912597656\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 14, train_loss=64.56250762939453\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 15, train_loss=53.313812255859375\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 16, train_loss=51.853511810302734\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 17, train_loss=48.70608139038086\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 18, train_loss=43.7628059387207\n",
      "\n",
      "For lr=0.1 and batch_size=64\n",
      ", Epoch = 19, train_loss=45.448455810546875\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 0, train_loss=1519.5767822265625\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 1, train_loss=549.9329223632812\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 2, train_loss=418.3136901855469\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 3, train_loss=227.72303771972656\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 4, train_loss=197.92401123046875\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 5, train_loss=153.6903076171875\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 6, train_loss=143.3049774169922\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 7, train_loss=129.6879119873047\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 8, train_loss=122.73651123046875\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 9, train_loss=108.62866973876953\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 10, train_loss=104.34671020507812\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 11, train_loss=97.79340362548828\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 12, train_loss=87.2995376586914\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 13, train_loss=78.85083770751953\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 14, train_loss=73.39517974853516\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 15, train_loss=69.93297576904297\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 16, train_loss=70.27364349365234\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 17, train_loss=65.49987030029297\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 18, train_loss=67.5092544555664\n",
      "\n",
      "For lr=0.1 and batch_size=128\n",
      ", Epoch = 19, train_loss=61.49169921875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# building hyperparameters grid to search\n",
    "lr_values = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "\n",
    "N_epochs = 20\n",
    "\n",
    "#INITIALIZING THE LOSS FOR LATER COMPARISON\n",
    "best_validate_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "loss_fun = nn.MSELoss() #Mean Square Loss\n",
    "losses_per_epoch = []\n",
    "raw_batch_loss = []\n",
    "for lr, batch_size in itertools.product(lr_values, batch_sizes):\n",
    "    mymodel_hyper = myMultiLayerPerceptron(8, 1)\n",
    "    mymodel_hyper.to(device)\n",
    "    #https://docs.python.org/3/library/itertools.html\n",
    "    optimizer = torch.optim.Adam(mymodel_hyper.parameters(), lr=lr)\n",
    "\n",
    "    train_dataloader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(N_epochs):\n",
    "        batch_loss = []\n",
    "        ###\n",
    "        # TRAINING LOOP\n",
    "        ###\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            prediction_score = mymodel_hyper(x_batch).squeeze(-1) \n",
    "            loss = loss_fun(prediction_score, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        # APPENDING LOSSES and R2\n",
    "        ###\n",
    "        losses_per_epoch.append(np.mean(np.array(batch_loss)))\n",
    "        raw_batch_loss.append(np.array(batch_loss))\n",
    "\n",
    "        if losses_per_epoch[-1] < best_validate_loss:\n",
    "            best_validate_loss = losses_per_epoch[-1]\n",
    "            print(\"Saving model...\\n\")\n",
    "            torch.save(mymodel_hyper.state_dict(), \"best_model\")\n",
    "            best_params = {\"learning_rate\": lr, \"batch_sizes\": batch_size}\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        print(f\"For lr={lr} and batch_size={batch_size}\\n, Epoch = {epoch}, train_loss={losses_per_epoch[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "c779e68e-bb4c-4942-abd5-39db4d523d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'batch_sizes': 8}"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce35cf3-a9cc-4d71-b404-0864852f625e",
   "metadata": {},
   "source": [
    "# Different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "38627569-e2d5-40d1-96dd-977128125ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING THE CLASS FOR THE MLP MODEL\n",
    "from torch import nn\n",
    "class myMultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(  # here we stack multiple layers together\n",
    "            nn.Linear(input_dim,16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8,4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4,output_dim)   \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y = self.sequential(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "6aea6ae2-30ec-4fed-9282-680256d3b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myMultiLayerPerceptron(\n",
      "  (sequential): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=4, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mymodel_hyper = myMultiLayerPerceptron(8, 1) # creating a model instance with input dimension 31 and output dimension 1\n",
    "mymodel_hyper.to(device)\n",
    "print(mymodel_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "d79d696b-8988-4bd8-82a4-afa09cb7c6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 0, train_loss=693.8685302734375\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 1, train_loss=143.84307861328125\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 2, train_loss=128.12342834472656\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 3, train_loss=132.2449493408203\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 4, train_loss=134.85699462890625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 5, train_loss=117.62342071533203\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 6, train_loss=119.76118469238281\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 7, train_loss=115.80081176757812\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 8, train_loss=115.8340835571289\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 9, train_loss=116.58826446533203\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 10, train_loss=109.46564483642578\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 11, train_loss=112.84386444091797\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 12, train_loss=121.55941009521484\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 13, train_loss=108.8392333984375\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 14, train_loss=101.42083740234375\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 15, train_loss=78.75704193115234\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 16, train_loss=62.0526123046875\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 17, train_loss=54.52254104614258\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 18, train_loss=50.162662506103516\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 19, train_loss=49.15041732788086\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 20, train_loss=46.75893783569336\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 21, train_loss=50.414024353027344\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 22, train_loss=47.46720504760742\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 23, train_loss=42.17528533935547\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 24, train_loss=41.860816955566406\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 25, train_loss=43.36929702758789\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 26, train_loss=46.46670150756836\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 27, train_loss=63.65707015991211\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 28, train_loss=43.702789306640625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 29, train_loss=40.12348556518555\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 30, train_loss=37.08353805541992\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 31, train_loss=37.36855697631836\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 32, train_loss=39.93906784057617\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 33, train_loss=39.284141540527344\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 34, train_loss=37.7674560546875\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 35, train_loss=37.898807525634766\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 36, train_loss=40.51792526245117\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 37, train_loss=37.55970001220703\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 38, train_loss=36.52427673339844\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 39, train_loss=36.55347442626953\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 40, train_loss=37.57475662231445\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 41, train_loss=37.55781173706055\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 42, train_loss=33.23958969116211\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 43, train_loss=34.96226501464844\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 44, train_loss=36.61772537231445\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 45, train_loss=35.77310562133789\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 46, train_loss=35.17340087890625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 47, train_loss=35.35487365722656\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 48, train_loss=33.626407623291016\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 49, train_loss=34.185638427734375\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 50, train_loss=32.55226516723633\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 51, train_loss=33.50703430175781\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 52, train_loss=33.4489631652832\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 53, train_loss=37.075679779052734\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 54, train_loss=34.617977142333984\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 55, train_loss=32.415809631347656\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 56, train_loss=43.32328796386719\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 57, train_loss=36.87022018432617\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 58, train_loss=32.4378662109375\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 59, train_loss=29.942312240600586\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 60, train_loss=34.12265396118164\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 61, train_loss=29.757999420166016\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 62, train_loss=30.65107536315918\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 63, train_loss=33.90497970581055\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 64, train_loss=38.40826416015625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 65, train_loss=29.20387840270996\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 66, train_loss=31.80028533935547\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 67, train_loss=29.16497802734375\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 68, train_loss=30.30344581604004\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 69, train_loss=29.839195251464844\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 70, train_loss=30.119537353515625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 71, train_loss=29.70253562927246\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 72, train_loss=29.44341278076172\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 73, train_loss=35.51197814941406\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 74, train_loss=30.75602912902832\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 75, train_loss=29.15031623840332\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 76, train_loss=29.02401351928711\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 77, train_loss=31.777803421020508\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 78, train_loss=32.874107360839844\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 79, train_loss=31.659616470336914\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 80, train_loss=28.26681900024414\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 81, train_loss=28.608997344970703\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 82, train_loss=28.049463272094727\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 83, train_loss=26.89629364013672\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 84, train_loss=34.275882720947266\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 85, train_loss=27.880290985107422\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 86, train_loss=27.43161392211914\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 87, train_loss=29.106491088867188\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 88, train_loss=29.97314453125\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 89, train_loss=28.144372940063477\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 90, train_loss=28.440757751464844\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 91, train_loss=31.86502456665039\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 92, train_loss=24.679885864257812\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 93, train_loss=26.32441520690918\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 94, train_loss=27.018733978271484\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 95, train_loss=25.25444793701172\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 96, train_loss=26.87891387939453\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 97, train_loss=28.460926055908203\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 98, train_loss=27.875558853149414\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 99, train_loss=29.184167861938477\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 100, train_loss=27.43187713623047\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 101, train_loss=28.222536087036133\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 102, train_loss=27.485864639282227\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 103, train_loss=26.950565338134766\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 104, train_loss=26.712501525878906\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 105, train_loss=26.62603759765625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 106, train_loss=24.77610206604004\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 107, train_loss=26.20362091064453\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 108, train_loss=25.794178009033203\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 109, train_loss=26.592012405395508\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 110, train_loss=28.599544525146484\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 111, train_loss=27.178682327270508\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 112, train_loss=25.746999740600586\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 113, train_loss=26.873165130615234\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 114, train_loss=25.517621994018555\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 115, train_loss=30.041576385498047\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 116, train_loss=28.263357162475586\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 117, train_loss=24.51352882385254\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 118, train_loss=23.743581771850586\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 119, train_loss=27.485239028930664\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 120, train_loss=24.09282684326172\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 121, train_loss=24.510644912719727\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 122, train_loss=26.88892936706543\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 123, train_loss=29.44504165649414\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 124, train_loss=26.325098037719727\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 125, train_loss=27.504310607910156\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 126, train_loss=23.436918258666992\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 127, train_loss=22.63330078125\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 128, train_loss=23.802663803100586\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 129, train_loss=26.309667587280273\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 130, train_loss=22.940547943115234\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 131, train_loss=24.80701446533203\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 132, train_loss=25.707111358642578\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 133, train_loss=22.156354904174805\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 134, train_loss=21.107440948486328\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 135, train_loss=23.957435607910156\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 136, train_loss=21.315563201904297\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 137, train_loss=23.93547248840332\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 138, train_loss=22.23686408996582\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 139, train_loss=22.179861068725586\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 140, train_loss=22.54161834716797\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 141, train_loss=21.678007125854492\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 142, train_loss=21.93956756591797\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 143, train_loss=20.841413497924805\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 144, train_loss=21.48012351989746\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 145, train_loss=22.995386123657227\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 146, train_loss=22.313447952270508\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 147, train_loss=19.972293853759766\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 148, train_loss=23.488025665283203\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 149, train_loss=19.557649612426758\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 150, train_loss=22.934226989746094\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 151, train_loss=20.592830657958984\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 152, train_loss=19.212121963500977\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 153, train_loss=19.050920486450195\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 154, train_loss=23.724239349365234\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 155, train_loss=18.9130916595459\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 156, train_loss=22.440839767456055\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 157, train_loss=22.268033981323242\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 158, train_loss=19.959928512573242\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 159, train_loss=20.110551834106445\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 160, train_loss=23.077537536621094\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 161, train_loss=18.453954696655273\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 162, train_loss=21.610883712768555\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 163, train_loss=19.337459564208984\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 164, train_loss=18.719730377197266\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 165, train_loss=21.70895767211914\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 166, train_loss=20.424930572509766\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 167, train_loss=20.76763916015625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 168, train_loss=26.433361053466797\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 169, train_loss=20.676706314086914\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 170, train_loss=22.446489334106445\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 171, train_loss=18.352108001708984\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 172, train_loss=16.708044052124023\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 173, train_loss=17.308931350708008\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 174, train_loss=17.96435546875\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 175, train_loss=19.014606475830078\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 176, train_loss=17.68590545654297\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 177, train_loss=17.018224716186523\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 178, train_loss=18.453378677368164\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 179, train_loss=18.702682495117188\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 180, train_loss=22.74187660217285\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 181, train_loss=17.34707260131836\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 182, train_loss=17.631025314331055\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 183, train_loss=17.644922256469727\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 184, train_loss=16.109516143798828\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 185, train_loss=17.1756534576416\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 186, train_loss=17.861282348632812\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 187, train_loss=17.752269744873047\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 188, train_loss=17.28352165222168\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 189, train_loss=16.316457748413086\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 190, train_loss=16.80990219116211\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 191, train_loss=17.97311782836914\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 192, train_loss=20.552034378051758\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 193, train_loss=18.221757888793945\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 194, train_loss=18.083921432495117\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 195, train_loss=16.90441131591797\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 196, train_loss=17.404226303100586\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 197, train_loss=21.655216217041016\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 198, train_loss=17.626602172851562\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 199, train_loss=17.46163558959961\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 200, train_loss=18.636138916015625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 201, train_loss=21.459596633911133\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 202, train_loss=17.779251098632812\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 203, train_loss=16.412635803222656\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 204, train_loss=16.61506462097168\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 205, train_loss=15.713362693786621\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 206, train_loss=15.988560676574707\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 207, train_loss=19.360910415649414\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 208, train_loss=14.77849292755127\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 209, train_loss=20.032827377319336\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 210, train_loss=20.749826431274414\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 211, train_loss=15.156824111938477\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 212, train_loss=15.367600440979004\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 213, train_loss=18.007999420166016\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 214, train_loss=18.395875930786133\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 215, train_loss=14.95056438446045\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 216, train_loss=18.174877166748047\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 217, train_loss=16.838241577148438\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 218, train_loss=18.472469329833984\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 219, train_loss=15.734403610229492\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 220, train_loss=16.518526077270508\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 221, train_loss=17.264949798583984\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 222, train_loss=15.94542121887207\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 223, train_loss=15.386059761047363\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 224, train_loss=17.37489891052246\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 225, train_loss=17.660531997680664\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 226, train_loss=16.982666015625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 227, train_loss=16.31995391845703\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 228, train_loss=17.399131774902344\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 229, train_loss=15.943591117858887\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 230, train_loss=16.254798889160156\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 231, train_loss=14.542738914489746\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 232, train_loss=16.39129638671875\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 233, train_loss=18.491384506225586\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 234, train_loss=15.502619743347168\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 235, train_loss=14.726892471313477\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 236, train_loss=14.772250175476074\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 237, train_loss=15.729888916015625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 238, train_loss=15.429627418518066\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 239, train_loss=15.926557540893555\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 240, train_loss=16.492944717407227\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 241, train_loss=16.256370544433594\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 242, train_loss=15.652539253234863\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 243, train_loss=15.167854309082031\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 244, train_loss=15.784311294555664\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 245, train_loss=14.856534957885742\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 246, train_loss=14.066758155822754\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 247, train_loss=15.558804512023926\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 248, train_loss=14.57218074798584\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 249, train_loss=14.894875526428223\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 250, train_loss=17.93360137939453\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 251, train_loss=15.633498191833496\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 252, train_loss=15.090252876281738\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 253, train_loss=15.313518524169922\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 254, train_loss=14.696443557739258\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 255, train_loss=16.117429733276367\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 256, train_loss=25.289146423339844\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 257, train_loss=16.089818954467773\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 258, train_loss=15.111137390136719\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 259, train_loss=16.28570556640625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 260, train_loss=17.044723510742188\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 261, train_loss=14.23734188079834\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 262, train_loss=14.561517715454102\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 263, train_loss=15.205979347229004\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 264, train_loss=15.490900993347168\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 265, train_loss=16.498559951782227\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 266, train_loss=14.742655754089355\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 267, train_loss=14.446585655212402\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 268, train_loss=13.5382080078125\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 269, train_loss=16.16370391845703\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 270, train_loss=15.083993911743164\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 271, train_loss=16.416126251220703\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 272, train_loss=16.651906967163086\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 273, train_loss=13.085864067077637\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 274, train_loss=16.95846176147461\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 275, train_loss=13.006053924560547\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 276, train_loss=14.712799072265625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 277, train_loss=15.206687927246094\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 278, train_loss=14.347381591796875\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 279, train_loss=15.321393966674805\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 280, train_loss=14.170275688171387\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 281, train_loss=14.921948432922363\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 282, train_loss=16.817434310913086\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 283, train_loss=13.056976318359375\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 284, train_loss=12.48302173614502\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 285, train_loss=13.501834869384766\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 286, train_loss=16.278339385986328\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 287, train_loss=14.711581230163574\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 288, train_loss=15.986282348632812\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 289, train_loss=17.86943244934082\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 290, train_loss=15.029960632324219\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 291, train_loss=13.211870193481445\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 292, train_loss=11.912571907043457\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 293, train_loss=14.779632568359375\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 294, train_loss=18.552940368652344\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 295, train_loss=14.464635848999023\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 296, train_loss=14.166900634765625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 297, train_loss=12.912010192871094\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 298, train_loss=14.914501190185547\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 299, train_loss=14.41439151763916\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 300, train_loss=14.27274227142334\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 301, train_loss=12.471612930297852\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 302, train_loss=13.852768898010254\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 303, train_loss=14.268304824829102\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 304, train_loss=13.50219440460205\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 305, train_loss=14.522176742553711\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 306, train_loss=12.53325366973877\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 307, train_loss=12.309184074401855\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 308, train_loss=12.309247970581055\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 309, train_loss=12.04781436920166\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 310, train_loss=13.702006340026855\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 311, train_loss=14.320120811462402\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 312, train_loss=14.110660552978516\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 313, train_loss=14.495027542114258\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 314, train_loss=14.351018905639648\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 315, train_loss=14.909994125366211\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 316, train_loss=12.917685508728027\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 317, train_loss=12.66250228881836\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 318, train_loss=13.243542671203613\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 319, train_loss=14.636283874511719\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 320, train_loss=12.677043914794922\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 321, train_loss=11.858892440795898\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 322, train_loss=14.581714630126953\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 323, train_loss=12.338164329528809\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 324, train_loss=12.768004417419434\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 325, train_loss=12.272001266479492\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 326, train_loss=13.460511207580566\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 327, train_loss=12.553383827209473\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 328, train_loss=13.86154842376709\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 329, train_loss=12.835227966308594\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 330, train_loss=13.065446853637695\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 331, train_loss=12.404251098632812\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 332, train_loss=11.809036254882812\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 333, train_loss=11.60033893585205\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 334, train_loss=13.066839218139648\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 335, train_loss=18.315597534179688\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 336, train_loss=24.862812042236328\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 337, train_loss=12.68584156036377\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 338, train_loss=10.881875038146973\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 339, train_loss=11.691825866699219\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 340, train_loss=12.212855339050293\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 341, train_loss=11.735956192016602\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 342, train_loss=13.264859199523926\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 343, train_loss=11.684075355529785\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 344, train_loss=11.037529945373535\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 345, train_loss=11.520233154296875\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 346, train_loss=12.103175163269043\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 347, train_loss=11.083843231201172\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 348, train_loss=13.134651184082031\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 349, train_loss=11.773009300231934\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 350, train_loss=11.8409423828125\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 351, train_loss=10.953343391418457\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 352, train_loss=12.084944725036621\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 353, train_loss=12.249481201171875\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 354, train_loss=13.014556884765625\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 355, train_loss=13.732242584228516\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 356, train_loss=11.979336738586426\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 357, train_loss=11.901126861572266\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 358, train_loss=13.053975105285645\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 359, train_loss=10.825148582458496\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 360, train_loss=11.600205421447754\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 361, train_loss=12.628273963928223\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 362, train_loss=11.414078712463379\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 363, train_loss=12.580183982849121\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 364, train_loss=14.927825927734375\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 365, train_loss=11.063254356384277\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 366, train_loss=11.518736839294434\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 367, train_loss=12.920838356018066\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 368, train_loss=12.222355842590332\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 369, train_loss=11.606114387512207\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 370, train_loss=11.617447853088379\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 371, train_loss=11.97777271270752\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 372, train_loss=11.138617515563965\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 373, train_loss=10.489002227783203\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 374, train_loss=12.320368766784668\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 375, train_loss=10.002636909484863\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 376, train_loss=10.740217208862305\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 377, train_loss=10.052521705627441\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 378, train_loss=9.740513801574707\n",
      "\n",
      "Saving model...\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 379, train_loss=11.046686172485352\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 380, train_loss=18.23855972290039\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 381, train_loss=18.161808013916016\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 382, train_loss=11.660529136657715\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 383, train_loss=11.409093856811523\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 384, train_loss=12.800169944763184\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 385, train_loss=11.302882194519043\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 386, train_loss=10.29883861541748\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 387, train_loss=11.664581298828125\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 388, train_loss=11.823446273803711\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 389, train_loss=10.91440486907959\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 390, train_loss=11.827013969421387\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 391, train_loss=11.451452255249023\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 392, train_loss=12.925602912902832\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 393, train_loss=12.337207794189453\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 394, train_loss=11.5771484375\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 395, train_loss=11.194591522216797\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 396, train_loss=9.906810760498047\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 397, train_loss=11.040017127990723\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 398, train_loss=12.617712020874023\n",
      "\n",
      "For lr=0.01 and batch_size=8\n",
      ", Epoch = 399, train_loss=12.282870292663574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# building hyperparameters grid to search\n",
    "#For lr=0.1 and batch_size=32\n",
    "lr = 0.01\n",
    "batch_size = 8\n",
    "N_epochs = 400\n",
    "\n",
    "#INITIALIZING THE LOSS FOR LATER COMPARISON\n",
    "best_validate_loss = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "loss_fun = nn.MSELoss() #Mean Square Loss\n",
    "\n",
    "optimizer = torch.optim.Adam(mymodel_hyper.parameters(), lr=lr)\n",
    "\n",
    "train_dataloader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "losses_per_epoch = []\n",
    "raw_batch_loss = []\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    batch_loss = []\n",
    "        ###\n",
    "        # TRAINING LOOP\n",
    "        ###\n",
    "    for x_batch, y_batch in train_dataloader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        prediction_score = mymodel_hyper(x_batch).squeeze(-1) \n",
    "        loss = loss_fun(prediction_score, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        # APPENDING LOSSES and R2\n",
    "        ###\n",
    "    losses_per_epoch.append(np.mean(np.array(batch_loss)))\n",
    "    raw_batch_loss.append(np.array(batch_loss))\n",
    "\n",
    "    print(f\"For lr={lr} and batch_size={batch_size}\\n, Epoch = {epoch}, train_loss={losses_per_epoch[-1]}\\n\")\n",
    "\n",
    "    if losses_per_epoch[-1] < best_validate_loss:\n",
    "        best_validate_loss = losses_per_epoch[-1]\n",
    "        print(\"Saving model...\\n\")\n",
    "        torch.save(mymodel_hyper.state_dict(), \"best_model\")\n",
    "        best_params = {\"learning_rate\": lr, \"batch_sizes\": batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "a19e9b4e-e5cd-4997-854d-3c6fa6240b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x39ed9d160>]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "###\n",
    "# Graph for train loss per Adam iteration\n",
    "###\n",
    "plt.figure(figsize = (12, 10))\n",
    "combined_array = np.concatenate(raw_batch_loss)\n",
    "x = range(1, len(combined_array)+1)\n",
    "y = combined_array\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Train loss per Adam iteration\")\n",
    "plt.xlabel(\"Number of training iterations\")\n",
    "plt.ylabel(\"Train loss\")\n",
    "\n",
    "###\n",
    "# Because there are so many iterations, I have added smoothing to show the general trend in yellow.\n",
    "###\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.uniform_filter1d.html\n",
    "smoothed_y = uniform_filter1d(y, size=200)\n",
    "plt.plot(x, smoothed_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "d7f66e4a-3982-4bc5-998f-e653972203f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c5/p6q0gjzj2ns5lv27n9yp07ph0000gn/T/ipykernel_43602/3185701737.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load(\"best_model\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = myMultiLayerPerceptron(8, 1)\n",
    "best_model.to(device)\n",
    "best_model.load_state_dict(torch.load(\"best_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "466826d2-c582-43c3-b8bf-a733b13a19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss=30.18462562561035, R2 = 0.8882950842380524\n"
     ]
    }
   ],
   "source": [
    "#RUNNING TESTING SET ON THE BEST MODEL SO FAR\n",
    "batch_size = 64\n",
    "test_dataloader = DataLoader(testing_set,batch_size = batch_size,shuffle = True)\n",
    "loss_fun = nn.MSELoss()\n",
    "targets = []\n",
    "predictions = []\n",
    "batch_loss = []\n",
    "with torch.no_grad():\n",
    "    best_model.eval()\n",
    "    for x_batch, y_batch in test_dataloader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        prediction_score = best_model(x_batch).squeeze(-1)\n",
    "        loss = loss_fun(prediction_score, y_batch)\n",
    "        batch_loss.append(loss.detach().cpu())\n",
    "        \n",
    "        targets.append(y_batch.cpu())\n",
    "        predictions.append(prediction_score.cpu())\n",
    "            \n",
    "###\n",
    "# APPENDNING LOSSES and R2\n",
    "###\n",
    "test_loss = np.mean(np.array(batch_loss))\n",
    "\n",
    "#CALCULATING THE R2\n",
    "targets_per_batch = torch.cat(targets).numpy()\n",
    "predictions_per_batch = torch.cat(predictions).numpy()\n",
    "ssr = np.sum((targets_per_batch - predictions_per_batch)**2)\n",
    "sst = np.sum((targets_per_batch - np.mean(targets_per_batch))**2)\n",
    "r2 = 1 - ssr/sst\n",
    "#https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html\n",
    "    \n",
    "print(f\"test_loss={test_loss}, R2 = {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9dbc1-d4d0-4f94-a6d3-774378f72c4a",
   "metadata": {},
   "source": [
    "# Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5146e91-2380-4dbf-920b-0e7b3f8622a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=2000, max_depth=60, random_state=0)\n",
    "regr.fit(X_train, y_train)\n",
    "prediction_train = regr.predict(X_train)\n",
    "prediction_test = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7ad4378-e257-4f3f-b38a-aaacab7b14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_train, prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30c275ce-cde2-4cc3-a07c-ea3c9677f74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.9102897843172784)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4550956-e9e9-4f61-8e4d-e218efb8d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = mean_squared_error(y_test, prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d1474ae-a667-488d-ba82-f5dbc76bc47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(30.366011313811658)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "313f30c0-908c-49fd-85cc-42baa4699b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8877735813425007"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, prediction_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c42917-033b-46b0-9d60-ee96be2ebfa2",
   "metadata": {},
   "source": [
    "# Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bf14ecc-44e4-4946-898c-1a593bfe3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(random_state=0, learning_rate=0.31, n_estimators=500)\n",
    "reg.fit(X_train, y_train)\n",
    "prediction_test_gb = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "344590f6-fdd6-4f1a-860d-07d6d590c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = mean_squared_error(y_test, prediction_test_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37c4194d-6904-417d-a09f-bcdf901b7414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(18.041025150906613)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3169d97b-96f3-4094-a603-9bf8da21b71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333241491392313"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, prediction_test_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a29543fc-b375-4dc5-8f13-6f82967ea95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11196106-d860-4250-8cce-bd354924126d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14518623,  0.46504884, -0.84713204, ..., -0.52651395,\n",
       "        -1.2925307 ,  5.05767679],\n",
       "       [ 0.77960154,  1.33474302, -0.84713204, ..., -0.36309858,\n",
       "        -0.22185751, -0.61233136],\n",
       "       [ 1.04095461,  1.33474302, -0.84713204, ..., -0.36309858,\n",
       "        -0.22185751, -0.27973311],\n",
       "       ...,\n",
       "       [-0.03987914, -0.85688631, -0.84713204, ..., -0.06328926,\n",
       "         1.02850909,  2.12764465],\n",
       "       [ 1.85564861, -0.85688631, -0.84713204, ..., -0.52651395,\n",
       "        -2.24091255, -0.27973311],\n",
       "       [-0.0380602 ,  0.27742681, -0.46473778, ...,  1.14238167,\n",
       "         0.11082486,  0.16373121]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7da8f3c6-6e30-4027-9725-fb46d570bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with different values\n",
    "test = np.array([266.0,114.0,0.0,228.0,0.0,932.0, 670.0, 90])\n",
    "test2 = np.array([190.3, 0.0, 125.2, 161.9, 9.9, 1088.1, 802.6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a14aa262-5d63-4055-863b-d75e21287ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63c53b80-87ca-4e58-af9d-4d6f4e149f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# creating DataFrame for test2 with the same columns as df_without_label\n",
    "test2_df = pd.DataFrame([test2], columns=df_without_label.columns)\n",
    "\n",
    "# transforming test data using the preprocessor\n",
    "test2_scaled = preprocessor.transform(test2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2579e5a3-9005-4b85-9afb-21c9c17f792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.86989053, -0.85688631,  1.11017627, -0.92134819,  0.61918296,\n",
       "         1.48207978,  0.36214605, -0.6756834 ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef2fbca0-9674-4b58-b934-3d7358beadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_testing = reg.predict(test2_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3e83a42-9b80-4977-a450-29bcdff4f04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(10.291289549610855)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the prediction from the model\n",
    "prediction_testing[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28849354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
